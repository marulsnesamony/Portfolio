{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3372702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Retrieve Youtube video details using Python and the Youtube Data API\n",
    "##### Followed the ideas from this video to parse the Youtube channel data - https://www.youtube.com/watch?v=SwSbnmqk3zY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88a2026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import seaborn as sb\n",
    "import os\n",
    "import csv\n",
    "import nltk ##natural language toolkit to get the library of commonly used words\n",
    "from nltk.corpus import stopwords\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0807713a",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = <<use the api key that was created>>\n",
    "\n",
    "#HandleList = [\"@SQLBI\", \"@Tableau\", \"@manutd\", \"@mancity\", \"@realmadrid\", \"@Databricks\" ]\n",
    "\n",
    "api_service_name = 'youtube'\n",
    "api_version = 'v3'\n",
    "\n",
    "ytdata = build(api_service_name, api_version, developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59dc1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Process code from here - function get_handle_stats retrieves the channel data using googleapi's channel list function \n",
    "## multiple Channel Handles are passed to this function and it retrieves the data needed from the response json\n",
    "\n",
    "def get_handle_stats(ytdata, HandleList):\n",
    "    \n",
    "    all_ytdata = []\n",
    "    \n",
    "    for handle in HandleList:\n",
    "        request = ytdata.channels().list(part=\"snippet,contentDetails,statistics\", \n",
    "            forHandle= handle)\n",
    "        response = request.execute()\n",
    "    \n",
    "    ## response is a json object with different details of the channel\n",
    "    ## parse the json response\n",
    "    ## playlistid below can be used to get the list of all videos uploaded to this channel\n",
    "    \n",
    "        ytinfo = dict(Channel = response['items'][0]['snippet']['title'],\n",
    "                      ChannelID = response['items'][0]['id'],\n",
    "                      ChannelImg = response['items'][0]['snippet']['thumbnails']['high']['url'],\n",
    "                      Subscribers = response['items'][0]['statistics']['subscriberCount'],\n",
    "                      Views = response['items'][0]['statistics']['viewCount'],\n",
    "                      Videos = response['items'][0]['statistics']['videoCount'],\n",
    "                      PlaylistID = response['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "                        )\n",
    "                   \n",
    "        all_ytdata.append(ytinfo)\n",
    "    \n",
    "    return all_ytdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f3ba85",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_stats = get_handle_stats(ytdata, HandleList)\n",
    "## channel_stats dictionary is converted to a dataframe for further processing\n",
    "channel_data = pd.DataFrame(channel_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf54975",
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_data['Subscribers'] = pd.to_numeric(channel_data['Subscribers'])\n",
    "channel_data['Views'] = pd.to_numeric(channel_data['Views'])\n",
    "channel_data['Videos'] = pd.to_numeric(channel_data['Videos'])\n",
    "#channel_data.dtypes\n",
    "channel_data\n",
    "#ChannelID and PlaylistID seems to give the same details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d4c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Verifying the values \n",
    "\n",
    "channel = channel_data.iloc[0][\"Channel\"]\n",
    "playlistid1 = channel_data.iloc[0][\"PlaylistID\"]\n",
    "len(channel_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1565365",
   "metadata": {},
   "outputs": [],
   "source": [
    "## function get_video_ids - Given a playlistid of a channel, all the videos uploaded in the channel is retrieved \n",
    "\n",
    "def get_video_ids(ytdata, playlistid):\n",
    "    \n",
    "    request = ytdata.playlistItems().list(\n",
    "        part=\"contentDetails\",\n",
    "        playlistId=playlistid,\n",
    "        maxResults=50\n",
    "    )\n",
    "    response = request.execute()\n",
    "    \n",
    "    video_ids = []\n",
    "    \n",
    "    for i in range(len(response['items'])):\n",
    "        video_ids.append(response['items'][i]['contentDetails']['videoId'])\n",
    "        \n",
    "    ## only 50 video ids can be retrieved in a single response, next_page_token is used as a pointer to retrieve the next 50\n",
    "    next_page_token = response.get('nextPageToken')    \n",
    "    more_pages = True\n",
    "    \n",
    "    while more_pages:\n",
    "        if next_page_token is None:\n",
    "            more_pages = False\n",
    "        else:\n",
    "            request = ytdata.playlistItems().list(\n",
    "                    part=\"contentDetails\",\n",
    "                    playlistId=playlistid,\n",
    "                    maxResults=50,\n",
    "                    pageToken=next_page_token\n",
    "                )\n",
    "            response = request.execute()\n",
    "\n",
    "            for i in range(len(response['items'])):\n",
    "                video_ids.append(response['items'][i]['contentDetails']['videoId'])\n",
    "\n",
    "            next_page_token = response.get('nextPageToken')                \n",
    "    return video_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffd1c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_details(ytdata, video_ids, channel, subscribers, ChannelImg):\n",
    "    \n",
    "    ##only 50 videos can be processed at time. To get all videoids in the channel we need to loop through\n",
    "    all_video_stats = []\n",
    "    \n",
    "    for i in range(0, len(video_ids), 50): \n",
    "        request = ytdata.videos().list(\n",
    "                part = \"snippet, statistics, contentDetails\",\n",
    "                id = ','.join(video_ids[i:i+50]))\n",
    "        response = request.execute()\n",
    "#    return response\n",
    "        \n",
    "        for video in response['items']:\n",
    "            video_stats = dict(Channel = channel,\n",
    "                               Subscribers = subscribers,\n",
    "                               ChannelImg = ChannelImg,\n",
    "                                Title = video['snippet']['title'],\n",
    "                                Video_id = video['id'],\n",
    "                                Published_time_UTC = video['snippet']['publishedAt'],\n",
    "                                VideoImg = video['snippet']['thumbnails']['default']['url'],\n",
    "                                 Duration = video['contentDetails']['duration'],\n",
    "                                Views = video['statistics']['viewCount'],\n",
    "                                Likes = video['statistics']['likeCount'],\n",
    "                                #Dislikes = video['statistics']['dislikeCount'],\n",
    "                                Favorites = video['statistics']['favoriteCount'],\n",
    "                                Comments = video['statistics']['commentCount']                           \n",
    "                              )\n",
    "            all_video_stats.append(video_stats)\n",
    "    \n",
    "    return all_video_stats\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae7bab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_video_details\n",
    "## The Published time returned by the Youtube API is in UTC timezone. (there is a Z at the end of the time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e49a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loop through all the PlaylistIDs of the Channels to get the entire list of Video Ids. \n",
    "\n",
    "def get_video_data(channel_data, ytdata):\n",
    "    video_data = pd.DataFrame()\n",
    "\n",
    "    for _, row in channel_data.iterrows():\n",
    "        playlist_id = row[\"PlaylistID\"]\n",
    "        channel = row[\"Channel\"]\n",
    "        subscribers = row['Subscribers']\n",
    "        ChannelImg = row['ChannelImg']\n",
    "        \n",
    "        # Get video IDs for the current playlist\n",
    "        video_ids = get_video_ids(ytdata, playlist_id)\n",
    "        \n",
    "        # Get details for each video\n",
    "        video_details = get_video_details(ytdata, video_ids, channel, subscribers, ChannelImg)\n",
    "        \n",
    "        # Append the new video details to the main DataFrame\n",
    "        video_data = pd.concat([video_data, pd.DataFrame(video_details)], ignore_index=True)\n",
    "\n",
    "    return video_data\n",
    "\n",
    "# for i in range(len(channel_data)):\n",
    "#     playlistid1 = channel_data.loc[i,\"PlaylistID\"]\n",
    "#     channel = channel_data.loc[i,\"Channel\"]\n",
    "#     video_ids = get_video_ids(ytdata, playlistid1)\n",
    "#     video_details = get_video_details(ytdata, video_ids, channel)\n",
    "#     video_data = pd.concat([video_data, pd.DataFrame(video_details)], ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3af0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_details = get_video_details(ytdata, video_ids, channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f42c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#video_data = pd.DataFrame(video_details)\n",
    "video_data = get_video_data(channel_data, ytdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0210981b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## The Published time returned by the Youtube API is in UTC timezone. (there is a Z at the end of the time)\n",
    "\n",
    "\n",
    "video_data['Published_datetime_UTC'] = pd.to_datetime(video_data['Published_time_UTC'])\n",
    "## The Published time returned by the Youtube API is in UTC timezone. (there is a Z at the end of the time)\n",
    "## Convert the time into local time\n",
    "video_data['Published_datetime'] = video_data['Published_datetime_UTC'].dt.tz_convert('America/Chicago')\n",
    "\n",
    "## Need to use tz_localize to remove the -+1:00 that appears in the timezone datetime\n",
    "\n",
    "video_data['Published_datetime'] = video_data['Published_datetime'].dt.tz_localize(None)\n",
    "\n",
    "video_data['Published_date'] = video_data['Published_datetime'].dt.date\n",
    "\n",
    "video_data['Subscribers'] = pd.to_numeric(video_data['Subscribers'])\n",
    "video_data['Views'] = pd.to_numeric(video_data['Views'])\n",
    "video_data['Likes'] = pd.to_numeric(video_data['Likes'])\n",
    "video_data['Favorites'] = pd.to_numeric(video_data['Favorites'])\n",
    "video_data['Comments'] = pd.to_numeric(video_data['Comments'])\n",
    "\n",
    "# - The first capture group extracts numbers after 'T' and before 'D'\n",
    "# - The second capture group extracts numbers after 'D' and before 'M'\n",
    "# - The third capture group extracts numbers after 'M' and before 'S'\n",
    "video_data[['Hours', 'Minutes', 'Seconds']] = video_data['Duration'].str.extract(r'PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?')\n",
    "\n",
    "# Convert the extracted columns to integers, set it to 0 if null (NaN)\n",
    "\n",
    "video_data['Hours'] = pd.to_numeric(video_data['Hours'], errors='coerce').fillna(0).astype(int)\n",
    "video_data['Minutes'] = pd.to_numeric(video_data['Minutes'], errors='coerce').fillna(0).astype(int)\n",
    "video_data['Seconds'] = pd.to_numeric(video_data['Seconds'], errors='coerce').fillna(0).astype(int)\n",
    "video_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f662cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select specific columns to be used as reference in Visualizations\n",
    "\n",
    "select_cols = ['Channel', 'Published_date']\n",
    "video_data_ref = video_data[video_data['Channel'] == 'SQLBI']\n",
    "video_data_ref = video_data_ref[select_cols].copy()  ## copy is needed to avoid SettingWithCopyWarning error on this df slice\n",
    "video_data_ref['Year'] = pd.to_datetime(video_data_ref['Published_date']).dt.year\n",
    "video_data_ref = video_data_ref.drop(columns = ['Published_date']).drop_duplicates()\n",
    "\n",
    "# Add 24 hrs to the df, this is only for TimeoftheDay Viz purposes:\n",
    "# Create a static list containing the hours of the day\n",
    "\n",
    "houroftheday = list(range(0, 24))\n",
    "video_data_ref = pd.concat([video_data_ref.assign(HouroftheDay = i) for i in houroftheday], ignore_index = True)\n",
    "\n",
    "video_data_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44520e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## folder to which the files will be written to\n",
    "os.chdir(\"../Data\")\n",
    "print(os.getcwd()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c65dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_data.to_csv(\"YTVideoDetails.csv\", mode='w', index=False)\n",
    "video_data_ref.to_csv(\"HourOfDay.txt\", mode='w', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca3e9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stopwords in the nltk library helps to identify the commonly used words like a, the, is, was etc\n",
    "# The Title of each video is parsed and these commonly used words are removed from the title\n",
    "# This is then used to identify the text/subject that is most often used in this channel's video topics\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_common_words(text):\n",
    "    if pd.isna(text):  # Handle NaN values\n",
    "        return \"\"\n",
    "    #words = text.translate(str.maketrans('', '', string.punctuation)).split()  # Remove punctuation & split\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]  # Remove stopwords\n",
    "    return ' '.join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769152d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter channels where word cloud analysis will be used\n",
    "wc_video_data = video_data[video_data['Channel'] == 'SQLBI']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8976711",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_video_data = wc_video_data[['Channel', 'Video_id', 'Title', 'Published_datetime']]\n",
    "# remove stop_words before exploding\n",
    "\n",
    "wc_video_data['Cleaned_Title'] = wc_video_data['Title'].apply(remove_common_words)\n",
    "wc_video_data['Cleaned_Title'] = wc_video_data['Cleaned_Title'].str.replace('#', ' #')\n",
    "\n",
    "wc_video_data = wc_video_data.assign(Title_words = wc_video_data['Cleaned_Title'].str.split()).explode('Title_words')\n",
    "#wc_video_data['Title_words'] = wc_video_data.assign(Title_words = wc_video_data['Cleaned_Title'].str.split())\n",
    "#wc_video_data['Title_words'] = wc_video_data['Title_words'].str.split('#')\n",
    "#wc_video_data = wc_video_data.explode('Title_words')\n",
    "\n",
    "wc_video_data = wc_video_data[['Channel', 'Video_id', 'Title', 'Published_datetime', 'Title_words']].drop_duplicates()\n",
    "\n",
    "wc_video_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e22317",
   "metadata": {},
   "outputs": [],
   "source": [
    "wc_video_data.to_csv(\"YTVideoTitleWordCloud.csv\", mode='w', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0b2b68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
